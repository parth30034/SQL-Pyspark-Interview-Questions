# ðŸ“š Related Repositories & Resources

A curated list of repos and resources that align with building a **SQL + PySpark scenario-based question bank**.

---

## 1. [mohankrishna02 / interview-scenerios-spark-sql](https://github.com/mohankrishna02/interview-scenerios-spark-sql)
- **What itâ€™s good for**: Real-world style scenario prompts and interview-style question formats.  
- **Whatâ€™s missing**: Some questions lack runnable notebooks or full sample datasets â€” good candidate to augment.  
- **Recommendation**: Fork as your **`scenario/`** folder; add `CREATE TABLE` + sample `INSERT` scripts per question.  
- **Tags**: `scenarios`, `sql`, `pyspark`, `interview`  
- **Difficulty**: `Intermediate â†’ Advanced`

---

## 2. [kailasneupane / LeetCode-SQL-PySpark-Solutions](https://github.com/kailasneupane/LeetCode-SQL-PySpark-Solutions)
- **What itâ€™s good for**: Runnable notebooks showing SQL â†’ PySpark translations; strong example of keeping solutions runnable.  
- **Whatâ€™s missing**: Primarily LeetCode problems (not long multi-step scenarios).  
- **Recommendation**: Borrow the **notebook layout** and runnable examples; use same pattern for scenario solutions.  
- **Tags**: `sql`, `pyspark`, `leetcode`, `notebooks`  
- **Difficulty**: `Basic â†’ Intermediate`

---

## 3. [areibman / pyspark_exercises](https://github.com/areibman/pyspark_exercises)
- **What itâ€™s good for**: Clear, small exercises covering PySpark API fundamentals (great for *basic* level).  
- **Whatâ€™s missing**: Fewer long end-to-end scenarios.  
- **Recommendation**: Use to fill **basic â†’ intermediate** levels and to teach primitives before scenario problems.  
- **Tags**: `pyspark`, `dataframe`, `basics`, `api`  
- **Difficulty**: `Basic`

---

## 4. [Devinterview-io / apache-spark-interview-questions](https://github.com/Devinterview-io/apache-spark-interview-questions)
- **What itâ€™s good for**: Curated Spark interview Q&A (conceptual coverage).  
- **Whatâ€™s missing**: Limited runnable code.  
- **Recommendation**: Include short **concept-check cards** in each scenario (e.g., *why broadcasting here?*) using this repo as source.  
- **Tags**: `concepts`, `interview`, `spark`, `q&a`  
- **Difficulty**: `Basic â†’ Advanced (conceptual)`

---

## 5. [gabridego / spark-exercises](https://github.com/gabridego/spark-exercises)
- **What itâ€™s good for**: Simulates real data-engineering scenarios for BI & pipelines.  
- **Whatâ€™s missing**: May need extra datasets or multiple-solution variants.  
- **Recommendation**: Adapt their **scenario templates** into your `scenario/advanced/` folder.  
- **Tags**: `scenarios`, `etl`, `bi`, `pipelines`  
- **Difficulty**: `Intermediate â†’ Advanced`

---

## 6. [VirtusLab / pyspark-workshop](https://github.com/VirtusLab/pyspark-workshop)
- **What itâ€™s good for**: Workshop-style notebooks and exercises; interactive learning format.  
- **Recommendation**: Use their **workshop-notebook style** for instructor-led sessions or group practice runs.  
- **Tags**: `workshop`, `pyspark`, `training`, `notebooks`  
- **Difficulty**: `Basic â†’ Intermediate`

---

## 7. [dhruv-agg / pyspark_practice](https://github.com/dhruv-agg/pyspark_practice)
- **What itâ€™s good for**: Solved PySpark data engineering exercises (time-series, cleaning, transforms).  
- **Recommendation**: Copy **test cases & sample datasets** into `datasets/` and convert them into `CREATE TABLE` scripts.  
- **Tags**: `practice`, `pyspark`, `data-cleaning`, `transformations`  
- **Difficulty**: `Intermediate`

---

## 8. [UrbanInstitute / pyspark-tutorials](https://github.com/UrbanInstitute/pyspark-tutorials)
- **What itâ€™s good for**: Well-documented tutorials with real datasets and reproducible examples.  
- **Recommendation**: Use their **documentation style** for writing problem statements + context.  
- **Tags**: `tutorials`, `pyspark`, `real-data`, `documentation`  
- **Difficulty**: `Basic â†’ Intermediate`

---

## 9. [LeetCode-SQL / leetcode-sql](https://github.com/topics/leetcode-sql)
- **What itâ€™s good for**: Large collection of SQL problems you can adapt to PySpark.  
- **Recommendation**: Pick challenging SQL problems and expand into **multi-step scenario prompts** (ingest â†’ transform â†’ aggregate â†’ serve).  
- **Tags**: `sql`, `leetcode`, `problem-bank`, `practice`  
- **Difficulty**: `Basic â†’ Advanced`

---

## 10. Additional Reading / Curated Lists
- **Sources**: Medium articles, interview prep blogs, curated Spark Q&A guides.  
- **Recommendation**: Copy the **common pitfalls**, **expected optimizations**, and **evaluation rubric** into each scenario for better guidance.  
- **Tags**: `reading`, `interview`, `optimizations`, `best-practices`  
- **Difficulty**: `All levels`
